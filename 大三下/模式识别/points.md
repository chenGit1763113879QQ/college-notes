# points

<details><summary>note</summary>
老师说会从这58个空中出29个考，但是我只找到了53个,经过补充现在有56个了
</details>


1. The common Math knowledge of pattern recognition includes [ ] , [ ] and [ ]
	1. Probability and Statistics
	2. Linear algebra
2. Pattern recogniztion-related disciplines include [ ] , [ ] and [ ]
	1. Artificial Intelligence
	2. Signal Processing,
	3. Information Theory,
	4. Digital Image Processing, Psychology, etc.
3. Typical pattern includes [ ] , [ ] , [ ] and [ ]
	1. Text, image;
	2. Speech, audio, video;
	3. ECG, EEG;
	4. Socioeconomic phenomenon, etc；
4. Typical application areas of pattern recognition  includes [ ] , [ ] , [ ] and [ ]
	1. Computer Vision
		1. Character Recognition (OCR)
		2. Traffic Sign Recognition (TSR)
		3. Action Recognition
	2. Human-Computer Interaction
		1. Speech Recognition
	3. Medical Field, Network Field, Financial Field, Robotics Field, Driverless Car Field
	4. Machine vision, Computer aided diagnosis, Biometrics, Image Data Base retrieval, Data mining Bionformatics
5. The tasks of pattern recognition include [ ] and [ ]
	1. Classification
	2. Regression
6. Methods for pattern classification include [ ] and [ ]
	1. Clustering
	2. statistical classification (Bayesian method)
	3. structure-based pattern recognition (HMM)
	4. neural network-based pattern recognition
7. Let $w_i$ denote the $i^{th}$ class and $x$ denote one sample, then the prior probability is definde as [ ] ,the class conditional probability is defined as [ ] , and the posterior probability is defined as [ ]
8. Let $w_i$ denote the $i^{th}$ class and $x$ denote one sample, then $p(w_i)$ is called [ ] ,$p(x|w_i)$ is called [ ] ,$p(w_i|x)$ is called [ ]
	1.  $p(w_i)$ prior probability
	2. $p(x|w_i)$ : class conditional probability
	3. $p(w_i|x)$ :  posterior probability
9. One way to represent a two class pattern classifier is called the [ ]
	1. discriminant function 𝑔(𝐱)
10. The classifier relying only on the linear discriminant functions is called [ ]
	1. linear classifier
11. The estimation methods of unknown probability density functions include [ ] and [ ]
	1. Parameter estimation
	2. Nonparametric estimation
12. For unknown probability density functions, two common parameter estimation methods include [ ] , and [ ]
	1. Maximum Likelihood Parameter Estimation
	2. Bayesian Estimation
13. For Support Vector Machine, the width between decision boundaries is called the [ ] , and few samples closest to the decision boundary are called [ ] , and the aim of Support Vector Machine is to find [ ]
	1. classification interval d (margin of classification)
	2. support vectors
	3. find the 𝐰 that can brings the maximum classification interval
14. For perception, the selection of learning rate Selection of $\rho$ include [ ], [ ] and [ ]
	1. Fixed increment ($ρ_k$ is a nonnegative)
	2. Absolute correction
	3. Partial correction
15. The common activation functions include [ ] , [ ] and [ ]
	1. sigmoid
	2. tanH
	3. ReLU
16. For an artificial neural network, the output is affected by [ ] , [ ] and [ ]
	1. input
	2. weight
	3. activation function
17. Decision tree model learning includes [ ] , [ ] and [ ]
	1. feature selection
	2. decision tree generation
	3. decision tree pruning
18. The common algotithm of decision tree are [ ] , [ ] and [ ]
	1. ID3, C4.5 and CART
19. If the original feature of a sample x is $x=(x_1,x_2,x_3)^T$,then making a decison on  using [ ] approach to achieve the reduced feature $x=(x_1-x_3,x_2-x_3)^T$, and using [ ] approach to achieve the reduced feature $x=(x_1,x_2)^T$
	1. **Feature extraction**:  Using the mapping (or transformation) methods to transform the original features to new features.
	2. **Feature selection**: Selecting the most representative and best performance features from the original features.
20. Two common approaches of dimension reduction are [ ] and [ ]
	1. K-L:
	2. PCA: Principal Component Analysis
21. For two orthogonal vectors $\varPhi_i$ and $\varPhi_j$,($i\neq j$), the result of $\varPhi_i \varPhi_j$ is [ ]
	1.  0
22. From the view of Math, K-L transformation is one common [ ] transformation
	1. orthogonal
23. For clustering method, different [ ] and [ ] can both lead to  different clusters
	1. Different similarity metric
	2. Different dimensional scale





